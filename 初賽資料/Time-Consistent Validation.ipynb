{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-27T10:26:31.480249Z",
     "start_time": "2025-09-27T10:01:59.424783Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Part 1: Feature Engineering (Final Corrected Version) ---\n",
    "print(\"--- Part 1: Feature Engineering (Final Corrected Version) ---\")\n",
    "\n",
    "# 1.1 Load Data\n",
    "print(\"Loading data...\")\n",
    "try:\n",
    "    df_trans = pd.read_csv('acct_transaction.csv')\n",
    "    df_alert = pd.read_csv('acct_alert.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: CSV files not found.\")\n",
    "    exit()\n",
    "\n",
    "# 1.2 Positive Sample Feature Engineering\n",
    "print(\"Generating features for positive samples...\")\n",
    "# (This part is correct and remains unchanged)\n",
    "alert_accts = df_alert['acct'].unique()\n",
    "acct_to_event_date = df_alert.set_index('acct')['event_date'].to_dict()\n",
    "positive_features_list = []\n",
    "for acct, event_date in tqdm(acct_to_event_date.items(), desc=\"Processing alerted accounts\"):\n",
    "    acct_txns = df_trans[((df_trans['from_acct'] == acct) | (df_trans['to_acct'] == acct)) & (df_trans['txn_date'] < event_date)].copy()\n",
    "    if acct_txns.empty: continue\n",
    "    features = {'acct': acct, 'label': 1, 'ref_date': event_date}\n",
    "    time_windows = [1, 3, 7, 30]\n",
    "    for window in time_windows:\n",
    "        window_txns = acct_txns[acct_txns['txn_date'] >= event_date - window]\n",
    "        from_txns = window_txns[window_txns['from_acct'] == acct]\n",
    "        features[f'from_count_{window}d'] = len(from_txns); features[f'from_amt_sum_{window}d'] = from_txns['txn_amt'].sum(); features[f'from_amt_mean_{window}d'] = from_txns['txn_amt'].mean()\n",
    "        to_txns = window_txns[window_txns['to_acct'] == acct]\n",
    "        features[f'to_count_{window}d'] = len(to_txns); features[f'to_amt_sum_{window}d'] = to_txns['txn_amt'].sum()\n",
    "    epsilon = 1e-6\n",
    "    features['from_amt_ratio_3d_30d'] = features['from_amt_sum_3d'] / (features['from_amt_sum_3d'] + epsilon)\n",
    "    features['from_count_ratio_3d_30d'] = features['from_count_3d'] / (features['from_count_3d'] + epsilon)\n",
    "    positive_features_list.append(features)\n",
    "positive_features_df = pd.DataFrame(positive_features_list).fillna(0)\n",
    "\n",
    "# 1.3 Negative Sample Feature Engineering (Corrected for Time-Split)\n",
    "print(\"\\nGenerating features for negative samples (Corrected for Time-Split)...\")\n",
    "all_accts = np.union1d(df_trans['from_acct'].unique(), df_trans['to_acct'].unique())\n",
    "non_alert_accts = np.setdiff1d(all_accts, alert_accts)\n",
    "neg_pos_ratio = 3\n",
    "num_negative_samples = len(positive_features_df) * neg_pos_ratio\n",
    "num_negative_samples = min(num_negative_samples, len(non_alert_accts))\n",
    "np.random.seed(42)\n",
    "selected_neg_accts = np.random.choice(non_alert_accts, size=num_negative_samples, replace=False)\n",
    "\n",
    "negative_features_list = []\n",
    "for acct in tqdm(selected_neg_accts, desc=\"Processing non-alerted accounts\"):\n",
    "    acct_all_txns = df_trans[(df_trans['from_acct'] == acct) | (df_trans['to_acct'] == acct)]\n",
    "    # **KEY CORRECTION**: Sample pseudo_event_date from the account's ENTIRE history\n",
    "    acct_txn_dates = acct_all_txns['txn_date'].unique()\n",
    "    if len(acct_txn_dates) < 2: continue\n",
    "    pseudo_event_date = np.random.choice(acct_txn_dates[acct_txn_dates > np.min(acct_txn_dates)])\n",
    "\n",
    "    acct_txns = acct_all_txns[acct_all_txns['txn_date'] < pseudo_event_date]\n",
    "    if acct_txns.empty: continue\n",
    "    features = {'acct': acct, 'label': 0, 'ref_date': pseudo_event_date}\n",
    "    time_windows = [1, 3, 7, 30]\n",
    "    for window in time_windows:\n",
    "        window_txns = acct_txns[acct_txns['txn_date'] >= pseudo_event_date - window]\n",
    "        from_txns = window_txns[window_txns['from_acct'] == acct]\n",
    "        features[f'from_count_{window}d'] = len(from_txns); features[f'from_amt_sum_{window}d'] = from_txns['txn_amt'].sum(); features[f'from_amt_mean_{window}d'] = from_txns['txn_amt'].mean()\n",
    "        to_txns = window_txns[window_txns['to_acct'] == acct]\n",
    "        features[f'to_count_{window}d'] = len(to_txns); features[f'to_amt_sum_{window}d'] = to_txns['txn_amt'].sum()\n",
    "    epsilon = 1e-6\n",
    "    features['from_amt_ratio_3d_30d'] = features['from_amt_sum_3d'] / (features['from_amt_sum_3d'] + epsilon)\n",
    "    features['from_count_ratio_3d_30d'] = features['from_count_3d'] / (features['from_count_3d'] + epsilon)\n",
    "    negative_features_list.append(features)\n",
    "negative_features_df = pd.DataFrame(negative_features_list).fillna(0)\n",
    "\n",
    "# 1.4 Combine\n",
    "full_features_df = pd.concat([positive_features_df, negative_features_df], ignore_index=True)\n",
    "print(f\"\\nFull feature dataset created. Total samples: {len(full_features_df)}\")\n",
    "\n",
    "# --- Part 2: Time-Split Validation ---\n",
    "print(\"\\n--- Part 2: Time-Split Validation ---\")\n",
    "split_date = 90\n",
    "train_df = full_features_df[full_features_df['ref_date'] <= split_date]\n",
    "val_df = full_features_df[full_features_df['ref_date'] > split_date]\n",
    "\n",
    "print(f\"Training set size: {len(train_df)} (ref_date <= {split_date})\")\n",
    "print(f\"Validation set size: {len(val_df)} (ref_date > {split_date})\")\n",
    "\n",
    "# Error handling for empty splits\n",
    "if len(train_df) == 0 or len(val_df) == 0 or 0 not in train_df['label'].value_counts() or 1 not in train_df['label'].value_counts() or 1 not in val_df['label'].value_counts():\n",
    "    print(\"\\nError: The data split resulted in an invalid training/validation set.\")\n",
    "    exit()\n",
    "\n",
    "print(f\"Training set label distribution:\\n{train_df['label'].value_counts()}\")\n",
    "print(f\"Validation set label distribution:\\n{val_df['label'].value_counts()}\")\n",
    "\n",
    "features_to_drop = ['acct', 'label', 'ref_date']\n",
    "X_train = train_df.drop(columns=features_to_drop)\n",
    "y_train = train_df['label']\n",
    "X_val = val_df.drop(columns=features_to_drop)\n",
    "y_val = val_df['label']\n",
    "X_val = X_val[X_train.columns]\n",
    "\n",
    "scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]\n",
    "print(f\"Calculated scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "lgb_params = {\n",
    "    'objective': 'binary', 'boosting_type': 'gbdt', 'n_estimators': 1000,\n",
    "    'learning_rate': 0.05, 'num_leaves': 31, 'max_depth': -1,\n",
    "    'seed': 42, 'n_jobs': -1, 'verbose': -1, 'scale_pos_weight': scale_pos_weight\n",
    "}\n",
    "model = lgb.LGBMClassifier(**lgb_params)\n",
    "model.fit(X_train, y_train, eval_set=[(X_val, y_val)], eval_metric='logloss', callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "\n",
    "val_preds_proba = model.predict_proba(X_val)[:, 1]\n",
    "best_f1, best_thresh = 0, 0.5\n",
    "for thresh in np.arange(0.1, 0.9, 0.01):\n",
    "    f1 = f1_score(y_val, (val_preds_proba > thresh).astype(int))\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        best_thresh = thresh\n",
    "\n",
    "print(\"\\n--- Time-Consistent Validation Result ---\")\n",
    "print(f\"F1-Score on the time-split validation set: {best_f1:.4f}\")\n",
    "print(f\"Found best threshold: {best_thresh:.2f}\")\n",
    "print(\"This F1-Score is our new reliable baseline.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Part 1: Feature Engineering (Final Corrected Version) ---\n",
      "Loading data...\n",
      "Generating features for positive samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing alerted accounts: 100%|██████████| 1004/1004 [06:07<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating features for negative samples (Corrected for Time-Split)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing non-alerted accounts: 100%|██████████| 2838/2838 [17:13<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full feature dataset created. Total samples: 1949\n",
      "\n",
      "--- Part 2: Time-Split Validation ---\n",
      "Training set size: 1286 (ref_date <= 90)\n",
      "Validation set size: 663 (ref_date > 90)\n",
      "Training set label distribution:\n",
      "label\n",
      "0    653\n",
      "1    633\n",
      "Name: count, dtype: int64\n",
      "Validation set label distribution:\n",
      "label\n",
      "0    350\n",
      "1    313\n",
      "Name: count, dtype: int64\n",
      "Calculated scale_pos_weight: 1.03\n",
      "\n",
      "--- Time-Consistent Validation Result ---\n",
      "F1-Score on the time-split validation set: 0.8477\n",
      "Found best threshold: 0.32\n",
      "This F1-Score is our new reliable baseline.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T10:27:54.858218Z",
     "start_time": "2025-09-27T10:27:51.659401Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- 假设 full_features_df, df_trans, X_train.columns (來自上一段程式碼) 已经存在 ---\n",
    "\n",
    "# --- 阶段一: 使用全量数据训练最终模型 ---\n",
    "print(\"--- Phase 1: Training Final Model on ALL Data ---\")\n",
    "\n",
    "# 准备全量训练数据\n",
    "X_full_train = full_features_df.drop(columns=['acct', 'label', 'ref_date'])\n",
    "y_full_train = full_features_df['label']\n",
    "X_full_train = X_full_train[X_train.columns] # 确保欄位順序与之前验证时一致\n",
    "\n",
    "# 计算全量数据的 scale_pos_weight\n",
    "scale_pos_weight = y_full_train.value_counts()[0] / y_full_train.value_counts()[1]\n",
    "print(f\"Training with {len(full_features_df)} samples. Scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# 使用我们验证过的 lgb_params 参数\n",
    "lgb_params = {\n",
    "    'objective': 'binary', 'boosting_type': 'gbdt', 'n_estimators': 500, # 使用固定的迭代次数\n",
    "    'learning_rate': 0.05, 'num_leaves': 31, 'max_depth': -1,\n",
    "    'seed': 42, 'n_jobs': -1, 'verbose': -1, 'scale_pos_weight': scale_pos_weight\n",
    "}\n",
    "\n",
    "# 训练最终模型\n",
    "final_model = lgb.LGBMClassifier(**lgb_params)\n",
    "final_model.fit(X_full_train, y_full_train)\n",
    "print(\"Final model training complete.\")\n",
    "\n",
    "\n",
    "# --- 阶段二: 为测试集生成特徵 (使用之前优化过的极速版) ---\n",
    "print(\"\\n--- Phase 2: Generating Features for Test Set (Vectorized) ---\")\n",
    "df_predict = pd.read_csv('acct_predict.csv')\n",
    "test_accts = df_predict['acct'].unique()\n",
    "latest_date = df_trans['txn_date'].max()\n",
    "\n",
    "test_features_df = pd.DataFrame(index=test_accts)\n",
    "test_features_df.index.name = 'acct'\n",
    "time_windows = [1, 3, 7, 30]\n",
    "\n",
    "for window in tqdm(time_windows, desc=\"Processing time windows\"):\n",
    "    window_trans = df_trans[df_trans['txn_date'] >= latest_date - window]\n",
    "    from_feats = window_trans.groupby('from_acct')['txn_amt'].agg(['count', 'sum', 'mean'])\n",
    "    from_feats.columns = [f'from_count_{window}d', f'from_amt_sum_{window}d', f'from_amt_mean_{window}d']\n",
    "    to_feats = window_trans.groupby('to_acct')['txn_amt'].agg(['count', 'sum'])\n",
    "    to_feats.columns = [f'to_count_{window}d', f'to_amt_sum_{window}d']\n",
    "    test_features_df = test_features_df.join(from_feats, how='left').join(to_feats, how='left')\n",
    "\n",
    "epsilon = 1e-6\n",
    "test_features_df['from_amt_ratio_3d_30d'] = test_features_df['from_amt_sum_3d'] / (test_features_df['from_amt_sum_30d'] + epsilon)\n",
    "test_features_df['from_count_ratio_3d_30d'] = test_features_df['from_count_3d'] / (test_features_df['from_count_30d'] + epsilon)\n",
    "test_features_df.fillna(0, inplace=True)\n",
    "test_features_df = test_features_df[X_train.columns] # 使用验证过的欄位顺序\n",
    "print(f\"Successfully generated features for {len(test_features_df)} test accounts.\")\n",
    "\n",
    "\n",
    "# --- 阶段三: 預測并使用可靠的阈值生成提交文件 ---\n",
    "print(\"\\n--- Phase 3: Predicting and Generating Final Submission File ---\")\n",
    "\n",
    "# 预测\n",
    "test_predictions_proba = final_model.predict_proba(test_features_df)[:, 1]\n",
    "\n",
    "# **关键**: 使用我们从可靠的验证集中找到的最佳阈值\n",
    "best_thresh_from_validation = 0.32\n",
    "print(f\"Using the reliable threshold found from our time-split validation: {best_thresh_from_validation}\")\n",
    "\n",
    "# 生成最终预测\n",
    "test_predictions_final = (test_predictions_proba > best_thresh_from_validation).astype(int)\n",
    "\n",
    "# 创建提交文件\n",
    "submission_df = pd.DataFrame({'acct': test_accts, 'label': test_predictions_final})\n",
    "submission_df.to_csv('submission_final.csv', index=False)\n",
    "\n",
    "print(\"\\n--- Submission File 'submission_final.csv' Generated ---\")\n",
    "print(f\"Prediction distribution:\\n{submission_df['label'].value_counts()}\")\n",
    "print(\"Preview of submission file:\")\n",
    "print(submission_df.head())"
   ],
   "id": "65ab14cf1345ccdd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1: Training Final Model on ALL Data ---\n",
      "Training with 1949 samples. Scale_pos_weight: 1.06\n",
      "Final model training complete.\n",
      "\n",
      "--- Phase 2: Generating Features for Test Set (Vectorized) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing time windows: 100%|██████████| 4/4 [00:02<00:00,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully generated features for 4780 test accounts.\n",
      "\n",
      "--- Phase 3: Predicting and Generating Final Submission File ---\n",
      "Using the reliable threshold found from our time-split validation: 0.32\n",
      "\n",
      "--- Submission File 'submission_final.csv' Generated ---\n",
      "Prediction distribution:\n",
      "label\n",
      "1    3452\n",
      "0    1328\n",
      "Name: count, dtype: int64\n",
      "Preview of submission file:\n",
      "                                                acct  label\n",
      "0  fcf31c5113d3dbd9cb5056045c6a0f213bd8a4fc1bc834...      1\n",
      "1  e21dfa45e990364194468e501fbfe52ec02a4b71a2e2e8...      0\n",
      "2  2552e943aaf9caa33183758cd40128ef20a6e6ff16c232...      1\n",
      "3  71700e7b7c3d40abdfdbcc7afc0752fa8d9bd28b408651...      1\n",
      "4  c70349fc718ffb88f03f31b5a7fcf65b33dd71dce6fee0...      1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 4
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
