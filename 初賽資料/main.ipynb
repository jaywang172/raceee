{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-09-27T02:21:55.663084Z",
     "start_time": "2025-09-27T02:21:38.694010Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def perform_data_profiling():\n",
    "    # --- 1. Load Data ---\n",
    "    print(\"--- 正在載入數據檔案 ---\")\n",
    "\n",
    "    # 嘗試載入交易紀錄檔案\n",
    "    try:\n",
    "        df_txn = pd.read_csv('acct_transaction.csv')\n",
    "        txn_rows = len(df_txn)\n",
    "        print(f\"成功載入 acct_transaction.csv，總行數為: {txn_rows}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"錯誤: acct_transaction.csv 檔案未找到。\")\n",
    "        return\n",
    "\n",
    "    # 嘗試載入警示帳戶檔案\n",
    "    try:\n",
    "        df_alert = pd.read_csv('acct_alert.csv')\n",
    "        alert_rows = len(df_alert)\n",
    "        print(f\"成功載入 acct_alert.csv，總行數為: {alert_rows}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"錯誤: acct_alert.csv 檔案未找到。\")\n",
    "        alert_rows = 0 # 即使未載入，我們仍繼續處理交易數據\n",
    "\n",
    "    print(\"\\n--- 2. 數據概況與指標計算 ---\")\n",
    "\n",
    "    # Q1: 總行數\n",
    "    print(f\"交易紀錄 (acct_transaction.csv) 總行數: {txn_rows}\")\n",
    "    print(f\"警示帳戶 (acct_alert.csv) 總行數: {alert_rows}\")\n",
    "\n",
    "    # Q2: 唯一的 from_acct 和 to_acct 數量\n",
    "    if 'from_acct' in df_txn.columns and 'to_acct' in df_txn.columns:\n",
    "        unique_from = df_txn['from_acct'].nunique()\n",
    "        unique_to = df_txn['to_acct'].nunique()\n",
    "        # 計算總體規模（不重複的發起和接收帳戶）\n",
    "        total_unique_accts = pd.concat([df_txn['from_acct'], df_txn['to_acct']]).nunique()\n",
    "        print(f\"\\nacct_transaction.csv 帳戶規模:\")\n",
    "        print(f\"  唯一的 from_acct (發起帳戶) 數量: {unique_from}\")\n",
    "        print(f\"  唯一的 to_acct (接收帳戶) 數量: {unique_to}\")\n",
    "        print(f\"  總體不重複帳戶 (from_acct/to_acct 合併) 數量: {total_unique_accts}\")\n",
    "    else:\n",
    "        print(\"\\n警告: acct_transaction.csv 中未找到 'from_acct' 或 'to_acct' 欄位。\")\n",
    "\n",
    "    # Q3: 'UNK' 或空值的佔比\n",
    "    cols_to_check = ['is_self_txn', 'currency_type', 'channel_type']\n",
    "    print(\"\\n欄位 'UNK' 或空值佔比 (acct_transaction.csv):\")\n",
    "    for col in cols_to_check:\n",
    "        if col in df_txn.columns:\n",
    "            total_count = len(df_txn)\n",
    "            # 計算空值 (NaN) 數量\n",
    "            null_count = df_txn[col].isnull().sum()\n",
    "            # 計算 'UNK' 數量 (大小寫不敏感處理)\n",
    "            unk_count = (df_txn[col].astype(str).str.upper() == 'UNK').sum()\n",
    "\n",
    "            combined_count = null_count + unk_count\n",
    "            combined_percentage = (combined_count / total_count) * 100\n",
    "\n",
    "            print(f\"  {col}: {combined_count} / {total_count} ({combined_percentage:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"  警告: acct_transaction.csv 中未找到欄位: {col}\")\n",
    "\n",
    "    # Q4: txn_date 欄位涵蓋的時間範圍\n",
    "    if 'txn_date' in df_txn.columns:\n",
    "        # 假設 txn_date 是代表天數的數值或字串，直接取 min/max\n",
    "        min_date = df_txn['txn_date'].min()\n",
    "        max_date = df_txn['txn_date'].max()\n",
    "\n",
    "        # 嘗試將其視為整數計算天數範圍\n",
    "        try:\n",
    "            day_range = int(max_date) - int(min_date) + 1\n",
    "        except (ValueError, TypeError):\n",
    "            day_range = \"無法計算 (非數值類型)\"\n",
    "\n",
    "        print(f\"\\ntxn_date 欄位涵蓋的時間範圍:\")\n",
    "        print(f\"  從第 {min_date} 天到第 {max_date} 天\")\n",
    "        print(f\"  總共涵蓋 {day_range} 天 (假設 txn_date 為連續天數代號)\")\n",
    "    else:\n",
    "        print(\"\\n警告: acct_transaction.csv 中未找到欄位: txn_date\")\n",
    "\n",
    "    print(\"\\n--- 數據概況分析完成 ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    perform_data_profiling()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 正在載入數據檔案 ---\n",
      "成功載入 acct_transaction.csv，總行數為: 4435890\n",
      "成功載入 acct_alert.csv，總行數為: 1004\n",
      "\n",
      "--- 2. 數據概況與指標計算 ---\n",
      "交易紀錄 (acct_transaction.csv) 總行數: 4435890\n",
      "警示帳戶 (acct_alert.csv) 總行數: 1004\n",
      "\n",
      "acct_transaction.csv 帳戶規模:\n",
      "  唯一的 from_acct (發起帳戶) 數量: 819399\n",
      "  唯一的 to_acct (接收帳戶) 數量: 1169482\n",
      "  總體不重複帳戶 (from_acct/to_acct 合併) 數量: 1800106\n",
      "\n",
      "欄位 'UNK' 或空值佔比 (acct_transaction.csv):\n",
      "  is_self_txn: 3346272 / 4435890 (75.44%)\n",
      "  currency_type: 0 / 4435890 (0.00%)\n",
      "  channel_type: 1701739 / 4435890 (38.36%)\n",
      "\n",
      "txn_date 欄位涵蓋的時間範圍:\n",
      "  從第 1 天到第 121 天\n",
      "  總共涵蓋 121 天 (假設 txn_date 為連續天數代號)\n",
      "\n",
      "--- 數據概況分析完成 ---\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T02:25:20.016822Z",
     "start_time": "2025-09-27T02:24:59.697095Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 步驟 1: 載入數據 ---\n",
    "# 請將 'path/to/your/acct_transaction.csv' 替換為您的檔案實際路徑\n",
    "try:\n",
    "    df_trans = pd.read_csv('acct_transaction.csv')\n",
    "    print(\"成功載入 acct_transaction.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"錯誤: acct_transaction.csv 檔案未找到。請確認檔案路徑是否正確。\")\n",
    "    # 如果檔案不在同一個目錄下，你需要提供完整路徑，例如:\n",
    "    # df_trans = pd.read_csv('C:/Users/YourUser/Downloads/acct_transaction.csv')\n",
    "\n",
    "# --- 接下來執行我之前提供的程式碼 ---\n",
    "\n",
    "# 為了效率，先對交易金額做對數轉換以處理極端值\n",
    "# 確保 df_trans 已成功載入\n",
    "if 'df_trans' in locals():\n",
    "    df_trans['log_txn_amt'] = np.log1p(df_trans['txn_amt'])\n",
    "\n",
    "    # 按 from_acct 進行聚合\n",
    "    print(\"正在按 from_acct 進行特徵聚合...\")\n",
    "    features_from = df_trans.groupby('from_acct').agg(\n",
    "        from_total_txn_count=('from_acct', 'count'),\n",
    "        from_total_txn_amt=('txn_amt', 'sum'),\n",
    "        from_mean_txn_amt=('txn_amt', 'mean'),\n",
    "        from_std_txn_amt=('txn_amt', 'std'),\n",
    "        from_distinct_to_acct=('to_acct', 'nunique')\n",
    "    ).fillna(0)\n",
    "    print(\"完成 'from_acct' 特徵聚合。\")\n",
    "\n",
    "    # 按 to_acct 進行聚合 (接收款項的特徵)\n",
    "    print(\"正在按 to_acct 進行特徵聚合...\")\n",
    "    features_to = df_trans.groupby('to_acct').agg(\n",
    "        to_total_txn_count=('to_acct', 'count'),\n",
    "        to_total_txn_amt=('txn_amt', 'sum'),\n",
    "        to_mean_txn_amt=('txn_amt', 'mean'),\n",
    "        to_std_txn_amt=('txn_amt', 'std'),\n",
    "        to_distinct_from_acct=('from_acct', 'nunique')\n",
    "    ).fillna(0)\n",
    "    print(\"完成 'to_acct' 特徵聚合。\")\n",
    "\n",
    "    # --- 建立完整的帳戶特徵表 ---\n",
    "    # 1. 獲取所有唯一帳戶\n",
    "    all_accounts_from = df_trans['from_acct'].unique()\n",
    "    all_accounts_to = df_trans['to_acct'].unique()\n",
    "    all_accounts = np.union1d(all_accounts_from, all_accounts_to)\n",
    "\n",
    "    # 2. 建立一個以所有帳戶為索引的 DataFrame\n",
    "    df_features = pd.DataFrame(index=all_accounts)\n",
    "    df_features.index.name = 'acct'\n",
    "\n",
    "    # 3. 合併特徵\n",
    "    print(\"正在合併所有帳戶特徵...\")\n",
    "    df_features = df_features.join(features_from, how='left')\n",
    "    df_features = df_features.join(features_to, how='left')\n",
    "\n",
    "    # 4. 將合併後產生的 NaN (代表該帳戶從未作為付款方或收款方) 填充為 0\n",
    "    df_features.fillna(0, inplace=True)\n",
    "\n",
    "    print(\"\\n--- 帳戶特徵工程完成 ---\")\n",
    "    print(f\"總共為 {len(df_features)} 個帳戶生成了特徵。\")\n",
    "    print(\"特徵表示範:\")\n",
    "    print(df_features.head())"
   ],
   "id": "daaa4da4da55fed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功載入 acct_transaction.csv\n",
      "正在按 from_acct 進行特徵聚合...\n",
      "完成 'from_acct' 特徵聚合。\n",
      "正在按 to_acct 進行特徵聚合...\n",
      "完成 'to_acct' 特徵聚合。\n",
      "正在合併所有帳戶特徵...\n",
      "\n",
      "--- 帳戶特徵工程完成 ---\n",
      "總共為 1800106 個帳戶生成了特徵。\n",
      "特徵表示範:\n",
      "                                                    from_total_txn_count  \\\n",
      "acct                                                                       \n",
      "00000577cfcd0bde8ee693021419ef13a1f7f933ec86262...                   0.0   \n",
      "00000eec52ea49377de91bc7b54eb3192943e6c20e0a514...                   1.0   \n",
      "000015150c92e2a41c4715a088df78d77a7d4f3017aadc3...                   1.0   \n",
      "00002846e6b430580825e2b10fe3ff1e3ddb93f42c608da...                   0.0   \n",
      "00002b3d8f9c7b91c407a5725849deb521fcf1dd5eea1ff...                   0.0   \n",
      "\n",
      "                                                    from_total_txn_amt  \\\n",
      "acct                                                                     \n",
      "00000577cfcd0bde8ee693021419ef13a1f7f933ec86262...                 0.0   \n",
      "00000eec52ea49377de91bc7b54eb3192943e6c20e0a514...               205.0   \n",
      "000015150c92e2a41c4715a088df78d77a7d4f3017aadc3...               585.0   \n",
      "00002846e6b430580825e2b10fe3ff1e3ddb93f42c608da...                 0.0   \n",
      "00002b3d8f9c7b91c407a5725849deb521fcf1dd5eea1ff...                 0.0   \n",
      "\n",
      "                                                    from_mean_txn_amt  \\\n",
      "acct                                                                    \n",
      "00000577cfcd0bde8ee693021419ef13a1f7f933ec86262...                0.0   \n",
      "00000eec52ea49377de91bc7b54eb3192943e6c20e0a514...              205.0   \n",
      "000015150c92e2a41c4715a088df78d77a7d4f3017aadc3...              585.0   \n",
      "00002846e6b430580825e2b10fe3ff1e3ddb93f42c608da...                0.0   \n",
      "00002b3d8f9c7b91c407a5725849deb521fcf1dd5eea1ff...                0.0   \n",
      "\n",
      "                                                    from_std_txn_amt  \\\n",
      "acct                                                                   \n",
      "00000577cfcd0bde8ee693021419ef13a1f7f933ec86262...               0.0   \n",
      "00000eec52ea49377de91bc7b54eb3192943e6c20e0a514...               0.0   \n",
      "000015150c92e2a41c4715a088df78d77a7d4f3017aadc3...               0.0   \n",
      "00002846e6b430580825e2b10fe3ff1e3ddb93f42c608da...               0.0   \n",
      "00002b3d8f9c7b91c407a5725849deb521fcf1dd5eea1ff...               0.0   \n",
      "\n",
      "                                                    from_distinct_to_acct  \\\n",
      "acct                                                                        \n",
      "00000577cfcd0bde8ee693021419ef13a1f7f933ec86262...                    0.0   \n",
      "00000eec52ea49377de91bc7b54eb3192943e6c20e0a514...                    1.0   \n",
      "000015150c92e2a41c4715a088df78d77a7d4f3017aadc3...                    1.0   \n",
      "00002846e6b430580825e2b10fe3ff1e3ddb93f42c608da...                    0.0   \n",
      "00002b3d8f9c7b91c407a5725849deb521fcf1dd5eea1ff...                    0.0   \n",
      "\n",
      "                                                    to_total_txn_count  \\\n",
      "acct                                                                     \n",
      "00000577cfcd0bde8ee693021419ef13a1f7f933ec86262...                 1.0   \n",
      "00000eec52ea49377de91bc7b54eb3192943e6c20e0a514...                 0.0   \n",
      "000015150c92e2a41c4715a088df78d77a7d4f3017aadc3...                 0.0   \n",
      "00002846e6b430580825e2b10fe3ff1e3ddb93f42c608da...                 1.0   \n",
      "00002b3d8f9c7b91c407a5725849deb521fcf1dd5eea1ff...                 1.0   \n",
      "\n",
      "                                                    to_total_txn_amt  \\\n",
      "acct                                                                   \n",
      "00000577cfcd0bde8ee693021419ef13a1f7f933ec86262...            4050.0   \n",
      "00000eec52ea49377de91bc7b54eb3192943e6c20e0a514...               0.0   \n",
      "000015150c92e2a41c4715a088df78d77a7d4f3017aadc3...               0.0   \n",
      "00002846e6b430580825e2b10fe3ff1e3ddb93f42c608da...            3050.0   \n",
      "00002b3d8f9c7b91c407a5725849deb521fcf1dd5eea1ff...              75.0   \n",
      "\n",
      "                                                    to_mean_txn_amt  \\\n",
      "acct                                                                  \n",
      "00000577cfcd0bde8ee693021419ef13a1f7f933ec86262...           4050.0   \n",
      "00000eec52ea49377de91bc7b54eb3192943e6c20e0a514...              0.0   \n",
      "000015150c92e2a41c4715a088df78d77a7d4f3017aadc3...              0.0   \n",
      "00002846e6b430580825e2b10fe3ff1e3ddb93f42c608da...           3050.0   \n",
      "00002b3d8f9c7b91c407a5725849deb521fcf1dd5eea1ff...             75.0   \n",
      "\n",
      "                                                    to_std_txn_amt  \\\n",
      "acct                                                                 \n",
      "00000577cfcd0bde8ee693021419ef13a1f7f933ec86262...             0.0   \n",
      "00000eec52ea49377de91bc7b54eb3192943e6c20e0a514...             0.0   \n",
      "000015150c92e2a41c4715a088df78d77a7d4f3017aadc3...             0.0   \n",
      "00002846e6b430580825e2b10fe3ff1e3ddb93f42c608da...             0.0   \n",
      "00002b3d8f9c7b91c407a5725849deb521fcf1dd5eea1ff...             0.0   \n",
      "\n",
      "                                                    to_distinct_from_acct  \n",
      "acct                                                                       \n",
      "00000577cfcd0bde8ee693021419ef13a1f7f933ec86262...                    1.0  \n",
      "00000eec52ea49377de91bc7b54eb3192943e6c20e0a514...                    0.0  \n",
      "000015150c92e2a41c4715a088df78d77a7d4f3017aadc3...                    0.0  \n",
      "00002846e6b430580825e2b10fe3ff1e3ddb93f42c608da...                    1.0  \n",
      "00002b3d8f9c7b91c407a5725849deb521fcf1dd5eea1ff...                    1.0  \n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-09-27T02:33:15.048078Z",
     "start_time": "2025-09-27T02:32:53.520749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# --- Assume df_features is the DataFrame we created in the previous step ---\n",
    "\n",
    "# --- Step 1: Load Labels and Prepare Training Data ---\n",
    "print(\"Loading labels and preparing training data...\")\n",
    "try:\n",
    "    df_alert = pd.read_csv('acct_alert.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: acct_alert.csv not found. Please check the file path.\")\n",
    "    # Stop execution if the file is not found\n",
    "    exit()\n",
    "\n",
    "# Create the training set by merging features with labels\n",
    "# We use a left merge to only include accounts that are in the alert list (our labeled data)\n",
    "# Note: For a real competition, we need ALL accounts, with non-alerted ones labeled as 0.\n",
    "# Let's construct the full training set.\n",
    "\n",
    "# Create a 'label' column on our main feature dataframe\n",
    "df_features['label'] = 0\n",
    "# Set the label to 1 for the accounts present in the alert file\n",
    "df_features.loc[df_features.index.isin(df_alert['acct']), 'label'] = 1\n",
    "\n",
    "# Now, df_features is our complete training dataset\n",
    "X = df_features.drop('label', axis=1)\n",
    "y = df_features['label']\n",
    "\n",
    "print(f\"Training data shape: {X.shape}\")\n",
    "print(f\"Label distribution:\\n{y.value_counts()}\")\n",
    "\n",
    "# --- Step 2: Set Up LightGBM and Cross-Validation ---\n",
    "# --- 步骤 2: Set Up LightGBM and Cross-Validation (再次修正) ---\n",
    "print(\"\\nSetting up LightGBM model and Stratified K-Fold Cross-Validation...\")\n",
    "\n",
    "scale_pos_weight = y.value_counts()[0] / y.value_counts()[1]\n",
    "print(f\"Calculated scale_pos_weight: {scale_pos_weight:.2f}\")\n",
    "\n",
    "# LightGBM 模型参数 (移除 'metric' 参数)\n",
    "lgb_params = {\n",
    "    'objective': 'binary',\n",
    "    # 'metric' 将在 .fit() 中定义\n",
    "    'boosting_type': 'gbdt',\n",
    "    'n_estimators': 1000,\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': -1,\n",
    "    'seed': 42,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1,\n",
    "    'scale_pos_weight': scale_pos_weight\n",
    "}\n",
    "\n",
    "# --- 步骤 3: 运行交叉验证循环 (修正部分) ---\n",
    "N_SPLITS = 5\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n",
    "f1_scores = []\n",
    "auc_scores = [] # 我们也追踪一下 AUC 分数\n",
    "\n",
    "print(f\"Starting training with {N_SPLITS}-Fold CV...\")\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"--- Fold {fold+1}/{N_SPLITS} ---\")\n",
    "    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "\n",
    "    model = lgb.LGBMClassifier(**lgb_params)\n",
    "\n",
    "    # 关键修正：在 fit 方法中明确指定 eval_metric 为 'logloss' 或 'auc'\n",
    "    model.fit(X_train, y_train,\n",
    "              eval_set=[(X_val, y_val)],\n",
    "              eval_metric='logloss', # 使用 logloss 作为早停的评估指标\n",
    "              callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "\n",
    "    val_preds_proba = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # 计算 AUC 分数\n",
    "    auc = roc_auc_score(y_val, val_preds_proba)\n",
    "    auc_scores.append(auc)\n",
    "\n",
    "    # 寻找最佳阈值以最大化 F1-score\n",
    "    best_f1 = 0\n",
    "    best_thresh = 0.5\n",
    "    for thresh in np.arange(0.1, 0.9, 0.01):\n",
    "        f1 = f1_score(y_val, (val_preds_proba > thresh).astype(int))\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_thresh = thresh\n",
    "\n",
    "    print(f\"Validation AUC for this fold: {auc:.4f}\")\n",
    "    print(f\"Best F1-score for this fold: {best_f1:.4f} at threshold {best_thresh:.2f}\")\n",
    "    f1_scores.append(best_f1)\n",
    "\n",
    "print(\"\\n--- Training Finished ---\")\n",
    "print(f\"Average AUC Score across {N_SPLITS} folds: {np.mean(auc_scores):.4f} (+/- {np.std(auc_scores):.4f})\")\n",
    "print(f\"Average F1-Score across {N_SPLITS} folds: {np.mean(f1_scores):.4f} (+/- {np.std(f1_scores):.4f})\")\n",
    "if np.mean(f1_scores) > 0.2:\n",
    "    print(f\"Our baseline model score is well above the 0.2 requirement.\")\n",
    "else:\n",
    "    print(\"The baseline model score is below the 0.2 requirement. We need to improve our features or model.\")"
   ],
   "id": "831bdac838018c30",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels and preparing training data...\n",
      "Training data shape: (1800106, 10)\n",
      "Label distribution:\n",
      "label\n",
      "0    1799102\n",
      "1       1004\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Setting up LightGBM model and Stratified K-Fold Cross-Validation...\n",
      "Calculated scale_pos_weight: 1791.93\n",
      "Starting training with 5-Fold CV...\n",
      "--- Fold 1/5 ---\n",
      "Validation AUC for this fold: 0.0560\n",
      "Best F1-score for this fold: 0.0003 at threshold 0.10\n",
      "--- Fold 2/5 ---\n",
      "Validation AUC for this fold: 0.0832\n",
      "Best F1-score for this fold: 0.0020 at threshold 0.83\n",
      "--- Fold 3/5 ---\n",
      "Validation AUC for this fold: 0.1131\n",
      "Best F1-score for this fold: 0.0035 at threshold 0.83\n",
      "--- Fold 4/5 ---\n",
      "Validation AUC for this fold: 0.0896\n",
      "Best F1-score for this fold: 0.0019 at threshold 0.76\n",
      "--- Fold 5/5 ---\n",
      "Validation AUC for this fold: 0.0598\n",
      "Best F1-score for this fold: 0.0008 at threshold 0.10\n",
      "\n",
      "--- Training Finished ---\n",
      "Average AUC Score across 5 folds: 0.0803 (+/- 0.0209)\n",
      "Average F1-Score across 5 folds: 0.0017 (+/- 0.0011)\n",
      "The baseline model score is below the 0.2 requirement. We need to improve our features or model.\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
