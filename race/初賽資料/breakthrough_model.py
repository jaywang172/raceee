"""
ğŸš€ BREAKTHROUGH MODEL - é‡å° F1=0.25+ çš„é©å‘½æ€§è§£æ±ºæ–¹æ¡ˆ
=============================================================

åŸºæ–¼æ·±åº¦åˆ†æä½ çš„æäº¤æ­·å²ï¼Œæˆ‘ç™¼ç¾:
âœ“ exp3 (0.1068): çŸ­æœŸçª—å£[1,3,7] + ç°¡å–®æ¯”ç‡ç‰¹å¾µ â†’ æœ€ä½³
âœ“ exp1 (0.1061): Precisionå„ªåŒ– â†’ æ¬¡ä½³
âœ“ graph (0.1040): åœ–ç‰¹å¾µæœ‰æ•ˆ
âœ— exp2 (0.0765): Z-score + é•·æœŸçª—å£[30,60] â†’ å¤§å¤±æ•—

é—œéµæ´å¯Ÿ:
1. è©é¨™å¸³æˆ¶ç‰¹å¾µåœ¨çŸ­æœŸå…§æœ€æ˜é¡¯ (1-7å¤©)
2. ç°¡å–®ç‰¹å¾µ > è¤‡é›œçµ±è¨ˆç‰¹å¾µ
3. åœ–çµæ§‹å¾ˆé‡è¦ä½†æœªè¢«å……åˆ†åˆ©ç”¨
4. éœ€è¦135%çš„æå‡ â†’ éœ€è¦é©å‘½æ€§æ–¹æ³•

çªç ´ç­–ç•¥:
1. äº¤æ˜“ç¶²çµ¡æ·±åº¦åˆ†æ (PageRank, é¢¨éšªå‚³æ’­)
2. æ™‚é–“åºåˆ—æ¨¡å¼ (äº¤æ˜“åºåˆ—ç‰¹å¾µ)
3. ç•°å¸¸å‚³æ’­ç®—æ³• (å¾å·²çŸ¥è©é¨™æ“´æ•£)
4. å¤šæ¨¡å‹æ·±åº¦Ensemble
5. é ˜åŸŸçŸ¥è­˜ç‰¹å¾µ (è©é¨™è¡Œç‚ºæ¨¡å¼)
"""

import pandas as pd
import numpy as np
import lightgbm as lgb
import xgboost as xgb
from tqdm import tqdm
from sklearn.metrics import f1_score, roc_auc_score
from sklearn.preprocessing import StandardScaler
from collections import defaultdict, Counter
import warnings
warnings.filterwarnings('ignore')

# ==================== é…ç½® ====================
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

# å¾exp3å­¸åˆ°ï¼šä½¿ç”¨çŸ­æœŸçª—å£
SHORT_WINDOWS = [1, 3, 7]  # æ ¸å¿ƒçª—å£
MEDIUM_WINDOWS = [14, 21]  # è¼”åŠ©çª—å£
NEG_POS_RATIO = 10  # å¢åŠ è² æ¨£æœ¬ä»¥æé«˜æ¨¡å‹æ³›åŒ–

# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šäº¤æ˜“ç¶²çµ¡æ·±åº¦åˆ†æ ====================

class TransactionNetworkAnalyzer:
    """äº¤æ˜“ç¶²çµ¡åˆ†æå™¨ - æ ¸å¿ƒçªç ´é»"""
    
    def __init__(self, df_trans):
        self.df_trans = df_trans
        self.graph = defaultdict(lambda: {'out': set(), 'in': set()})
        self.build_graph()
    
    def build_graph(self):
        """æ§‹å»ºæœ‰å‘åœ–"""
        print("æ§‹å»ºäº¤æ˜“ç¶²çµ¡åœ–...")
        for _, row in tqdm(self.df_trans.iterrows(), total=len(self.df_trans)):
            from_acct = row['from_acct']
            to_acct = row['to_acct']
            self.graph[from_acct]['out'].add(to_acct)
            self.graph[to_acct]['in'].add(from_acct)
    
    def compute_pagerank(self, iterations=20, damping=0.85):
        """è¨ˆç®—PageRankåˆ†æ•¸ - è­˜åˆ¥æ ¸å¿ƒè©é¨™ç¯€é»"""
        print("è¨ˆç®—PageRankåˆ†æ•¸...")
        nodes = list(self.graph.keys())
        n = len(nodes)
        
        # åˆå§‹åŒ–
        pagerank = {node: 1.0 / n for node in nodes}
        
        for _ in range(iterations):
            new_pagerank = {}
            for node in nodes:
                rank = (1 - damping) / n
                
                # ä¾†è‡ªå…¥é‚Šçš„è²¢ç»
                for in_node in self.graph[node]['in']:
                    out_degree = len(self.graph[in_node]['out'])
                    if out_degree > 0:
                        rank += damping * pagerank[in_node] / out_degree
                
                new_pagerank[node] = rank
            
            pagerank = new_pagerank
        
        return pagerank
    
    def compute_risk_propagation(self, alert_accts, iterations=5):
        """é¢¨éšªå‚³æ’­ç®—æ³• - å¾å·²çŸ¥è©é¨™å¸³æˆ¶æ“´æ•£é¢¨éšª"""
        print("è¨ˆç®—é¢¨éšªå‚³æ’­åˆ†æ•¸...")
        
        # åˆå§‹åŒ–ï¼šalertå¸³æˆ¶é¢¨éšª=1ï¼Œå…¶ä»–=0
        risk_score = defaultdict(float)
        for acct in alert_accts:
            risk_score[acct] = 1.0
        
        # è¿­ä»£å‚³æ’­
        for iteration in range(iterations):
            new_risk = defaultdict(float)
            
            for acct in self.graph.keys():
                # å¾è½‰å…¥å¸³æˆ¶ç²å¾—é¢¨éšªï¼ˆæ”¶åˆ°è©é¨™æ¬¾ï¼‰
                for in_acct in self.graph[acct]['in']:
                    new_risk[acct] += risk_score[in_acct] * 0.6  # å¼·å‚³æ’­
                
                # å¾è½‰å‡ºå¸³æˆ¶ç²å¾—é¢¨éšªï¼ˆè½‰çµ¦è©é¨™å¸³æˆ¶ï¼‰
                for out_acct in self.graph[acct]['out']:
                    new_risk[acct] += risk_score[out_acct] * 0.4  # å¼±å‚³æ’­
                
                # ä¿ç•™åŸæœ‰é¢¨éšª
                new_risk[acct] = max(new_risk[acct], risk_score[acct] * 0.9)
            
            risk_score = new_risk
        
        return dict(risk_score)
    
    def compute_network_features(self, acct):
        """è¨ˆç®—å–®å€‹å¸³æˆ¶çš„ç¶²çµ¡ç‰¹å¾µ"""
        features = {}
        
        # åŸºç¤åº¦æ•¸
        features['out_degree'] = len(self.graph[acct]['out'])
        features['in_degree'] = len(self.graph[acct]['in'])
        features['total_degree'] = features['out_degree'] + features['in_degree']
        
        # åº¦æ•¸æ¯”ç‡ï¼ˆè©é¨™å¸³æˆ¶é€šå¸¸å‡ºåº¦>>å…¥åº¦ï¼‰
        features['degree_ratio'] = features['out_degree'] / (features['in_degree'] + 1e-8)
        
        # äºŒéšé„°å±…
        second_order_out = set()
        for neighbor in self.graph[acct]['out']:
            second_order_out.update(self.graph[neighbor]['out'])
        features['second_order_out'] = len(second_order_out)
        
        # å…±åŒé„°å±…ï¼ˆä¸­ç¹¼æ¨¡å¼ï¼‰
        features['common_neighbors'] = len(
            self.graph[acct]['out'].intersection(self.graph[acct]['in'])
        )
        
        return features


# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šæ™‚åºæ¨¡å¼åˆ†æ ====================

class TemporalPatternExtractor:
    """æ™‚åºæ¨¡å¼æå–å™¨"""
    
    def __init__(self, df_trans):
        self.df_trans = df_trans
    
    def extract_sequence_features(self, acct, ref_date, lookback=30):
        """æå–äº¤æ˜“åºåˆ—ç‰¹å¾µ"""
        features = {}
        
        # ç²å–è©²å¸³æˆ¶çš„äº¤æ˜“åºåˆ—
        acct_txns = self.df_trans[
            ((self.df_trans['from_acct'] == acct) | (self.df_trans['to_acct'] == acct)) &
            (self.df_trans['txn_date'] < ref_date) &
            (self.df_trans['txn_date'] >= ref_date - lookback)
        ].sort_values('txn_date')
        
        if len(acct_txns) == 0:
            return self._empty_sequence_features()
        
        # 1. äº¤æ˜“æ™‚é–“é–“éš”æ¨¡å¼
        date_diffs = acct_txns['txn_date'].diff().dropna()
        if len(date_diffs) > 0:
            features['avg_interval'] = date_diffs.mean()
            features['std_interval'] = date_diffs.std()
            features['min_interval'] = date_diffs.min()
            features['interval_cv'] = features['std_interval'] / (features['avg_interval'] + 1e-8)
        
        # 2. é‡‘é¡åºåˆ—æ¨¡å¼
        amounts = acct_txns['txn_amt'].values
        if len(amounts) > 1:
            # è¶¨å‹¢ï¼šæ˜¯å¦é‡‘é¡éå¢ï¼ˆå…¸å‹è©é¨™æ¨¡å¼ï¼‰
            features['amount_trend'] = np.corrcoef(np.arange(len(amounts)), amounts)[0, 1]
            
            # è®Šç•°ä¿‚æ•¸
            features['amount_cv'] = np.std(amounts) / (np.mean(amounts) + 1e-8)
            
            # æœ€å¤§é‡‘é¡ä½”æ¯”ï¼ˆå–®ç­†å¤§é¡ï¼‰
            features['max_amount_ratio'] = np.max(amounts) / (np.sum(amounts) + 1e-8)
        
        # 3. äº¤æ˜“æ™‚é–“æ¨¡å¼ï¼ˆè©é¨™å¸¸åœ¨å¤œé–“/é€±æœ«ï¼‰
        try:
            times = pd.to_datetime(acct_txns['txn_time'], format='%H:%M:%S', errors='coerce')
            hours = times.dt.hour
            
            features['night_txn_ratio'] = ((hours >= 0) & (hours < 6)).mean()
            features['evening_txn_ratio'] = ((hours >= 22) | (hours < 2)).mean()
            features['business_hour_ratio'] = ((hours >= 9) & (hours <= 17)).mean()
        except:
            features['night_txn_ratio'] = 0
            features['evening_txn_ratio'] = 0
            features['business_hour_ratio'] = 0
        
        # 4. çªç™¼æ€§ï¼ˆçŸ­æœŸå…§å¯†é›†äº¤æ˜“ï¼‰
        recent_3d = acct_txns[acct_txns['txn_date'] >= ref_date - 3]
        features['burst_ratio'] = len(recent_3d) / (len(acct_txns) + 1e-8)
        
        # 5. é€šè·¯å¤šæ¨£æ€§ï¼ˆè©é¨™å¯èƒ½ä½¿ç”¨å¤šç¨®é€šè·¯ï¼‰
        features['channel_diversity'] = acct_txns['channel_type'].nunique()
        features['channel_entropy'] = self._calculate_entropy(
            acct_txns['channel_type'].value_counts(normalize=True)
        )
        
        return features
    
    def _empty_sequence_features(self):
        """ç©ºåºåˆ—ç‰¹å¾µ"""
        return {
            'avg_interval': 0, 'std_interval': 0, 'min_interval': 0, 'interval_cv': 0,
            'amount_trend': 0, 'amount_cv': 0, 'max_amount_ratio': 0,
            'night_txn_ratio': 0, 'evening_txn_ratio': 0, 'business_hour_ratio': 0,
            'burst_ratio': 0, 'channel_diversity': 0, 'channel_entropy': 0
        }
    
    @staticmethod
    def _calculate_entropy(probs):
        """è¨ˆç®—ç†µ"""
        return -np.sum(probs * np.log(probs + 1e-8))


# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šé ˜åŸŸçŸ¥è­˜ç‰¹å¾µ ====================

class FraudDomainFeatures:
    """è©é¨™é ˜åŸŸç‰¹å¾µå·¥ç¨‹"""
    
    @staticmethod
    def extract_fraud_patterns(df_trans, acct, ref_date):
        """æå–è©é¨™è¡Œç‚ºæ¨¡å¼ç‰¹å¾µ"""
        features = {}
        
        acct_txns = df_trans[
            ((df_trans['from_acct'] == acct) | (df_trans['to_acct'] == acct)) &
            (df_trans['txn_date'] < ref_date)
        ]
        
        if len(acct_txns) == 0:
            return {k: 0 for k in [
                'round_amount_ratio', 'atm_preference', 'self_txn_ratio',
                'foreign_currency_ratio', 'quick_transfer_ratio',
                'concentration_index', 'velocity_score'
            ]}
        
        # 1. æ•´æ•¸é‡‘é¡åå¥½ï¼ˆè©é¨™å¸¸ç”¨æ•´æ•¸ï¼‰
        amounts = acct_txns['txn_amt'].values
        features['round_amount_ratio'] = np.mean(amounts % 1000 == 0)
        
        # 2. ATMåå¥½ï¼ˆè©é¨™å¸¸ç”¨ATMå–æ¬¾ï¼‰
        features['atm_preference'] = (acct_txns['channel_type'] == '1').mean()
        
        # 3. è‡ªæˆ‘äº¤æ˜“æ¯”ä¾‹
        features['self_txn_ratio'] = (acct_txns['is_self_txn'] == 'Y').mean()
        
        # 4. å¤–å¹£äº¤æ˜“ï¼ˆå¯èƒ½æ¶‰åŠæ´—éŒ¢ï¼‰
        features['foreign_currency_ratio'] = (acct_txns['currency_type'] != 'TWD').mean()
        
        # 5. å¿«é€Ÿè½‰ç§»ï¼ˆè³‡é‡‘å¿«é€²å¿«å‡ºï¼‰
        from_txns = acct_txns[acct_txns['from_acct'] == acct]
        to_txns = acct_txns[acct_txns['to_acct'] == acct]
        
        recent_7d_from = from_txns[from_txns['txn_date'] >= ref_date - 7]
        recent_7d_to = to_txns[to_txns['txn_date'] >= ref_date - 7]
        
        if len(to_txns) > 0:
            features['quick_transfer_ratio'] = len(recent_7d_from) / (len(to_txns) + 1e-8)
        else:
            features['quick_transfer_ratio'] = 0
        
        # 6. äº¤æ˜“å°æ‰‹é›†ä¸­åº¦ï¼ˆHerfindahlæŒ‡æ•¸ï¼‰
        if len(from_txns) > 0:
            counterparty_counts = from_txns['to_acct'].value_counts(normalize=True)
            features['concentration_index'] = (counterparty_counts ** 2).sum()
        else:
            features['concentration_index'] = 0
        
        # 7. è³‡é‡‘æµé€Ÿåº¦ï¼ˆçŸ­æœŸå…§å¤§é‡é€²å‡ºï¼‰
        features['velocity_score'] = (
            recent_7d_from['txn_amt'].sum() + recent_7d_to['txn_amt'].sum()
        ) / 7.0
        
        return features


# ==================== ç¬¬å››éƒ¨åˆ†ï¼šä¸»ç‰¹å¾µå·¥ç¨‹ç®¡é“ ====================

def engineer_breakthrough_features(df_trans, df_alert, samples):
    """å·¥ç¨‹åŒ–çªç ´æ€§ç‰¹å¾µé›†"""
    
    print("\n" + "="*80)
    print("é–‹å§‹é©å‘½æ€§ç‰¹å¾µå·¥ç¨‹")
    print("="*80)
    
    # åˆå§‹åŒ–åˆ†æå™¨
    network_analyzer = TransactionNetworkAnalyzer(df_trans)
    temporal_extractor = TemporalPatternExtractor(df_trans)
    
    # è¨ˆç®—å…¨å±€ç¶²çµ¡ç‰¹å¾µ
    print("\nè¨ˆç®—å…¨å±€ç¶²çµ¡ç‰¹å¾µ...")
    pagerank_scores = network_analyzer.compute_pagerank()
    alert_accts = df_alert['acct'].unique()
    risk_scores = network_analyzer.compute_risk_propagation(alert_accts)
    
    # ç‚ºæ¯å€‹æ¨£æœ¬æå–ç‰¹å¾µ
    feature_list = []
    
    for acct, ref_date, label in tqdm(samples, desc="æå–ç‰¹å¾µ"):
        features = {'acct': acct, 'label': label, 'ref_date': ref_date}
        
        # 1. åŸºç¤çµ±è¨ˆç‰¹å¾µï¼ˆå¾exp3å­¸åˆ°ï¼šä½¿ç”¨çŸ­æœŸçª—å£ï¼‰
        acct_txns = df_trans[
            ((df_trans['from_acct'] == acct) | (df_trans['to_acct'] == acct)) &
            (df_trans['txn_date'] < ref_date)
        ]
        
        for window in SHORT_WINDOWS + MEDIUM_WINDOWS:
            window_txns = acct_txns[acct_txns['txn_date'] >= ref_date - window]
            
            from_txns = window_txns[window_txns['from_acct'] == acct]
            features[f'from_count_{window}d'] = len(from_txns)
            features[f'from_amt_sum_{window}d'] = from_txns['txn_amt'].sum()
            features[f'from_amt_mean_{window}d'] = from_txns['txn_amt'].mean() if len(from_txns) > 0 else 0
            features[f'from_amt_std_{window}d'] = from_txns['txn_amt'].std() if len(from_txns) > 0 else 0
            features[f'from_unique_{window}d'] = from_txns['to_acct'].nunique()
            
            to_txns = window_txns[window_txns['to_acct'] == acct]
            features[f'to_count_{window}d'] = len(to_txns)
            features[f'to_amt_sum_{window}d'] = to_txns['txn_amt'].sum()
            features[f'to_amt_mean_{window}d'] = to_txns['txn_amt'].mean() if len(to_txns) > 0 else 0
            features[f'to_unique_{window}d'] = to_txns['from_acct'].nunique()
        
        # 2. æ¯”ç‡ç‰¹å¾µï¼ˆå¾exp3å­¸åˆ°ï¼šç°¡å–®æœ‰æ•ˆï¼‰
        epsilon = 1e-8
        features['ratio_from_amt_3d_7d'] = features['from_amt_sum_3d'] / (features['from_amt_sum_7d'] + epsilon)
        features['ratio_from_amt_7d_14d'] = features['from_amt_sum_7d'] / (features['from_amt_sum_14d'] + epsilon)
        features['ratio_from_cnt_1d_7d'] = features['from_count_1d'] / (features['from_count_7d'] + epsilon)
        features['ratio_to_amt_3d_7d'] = features['to_amt_sum_3d'] / (features['to_amt_sum_7d'] + epsilon)
        
        # 3. ç¶²çµ¡ç‰¹å¾µï¼ˆçªç ´é»1ï¼‰
        network_feats = network_analyzer.compute_network_features(acct)
        features.update(network_feats)
        features['pagerank_score'] = pagerank_scores.get(acct, 0)
        features['risk_propagation_score'] = risk_scores.get(acct, 0)
        
        # 4. æ™‚åºç‰¹å¾µï¼ˆçªç ´é»2ï¼‰
        temporal_feats = temporal_extractor.extract_sequence_features(acct, ref_date)
        features.update(temporal_feats)
        
        # 5. é ˜åŸŸçŸ¥è­˜ç‰¹å¾µï¼ˆçªç ´é»3ï¼‰
        domain_feats = FraudDomainFeatures.extract_fraud_patterns(df_trans, acct, ref_date)
        features.update(domain_feats)
        
        # 6. äº¤äº’ç‰¹å¾µ
        features['risk_x_degree'] = features['risk_propagation_score'] * features['total_degree']
        features['pagerank_x_out_degree'] = features['pagerank_score'] * features['out_degree']
        features['velocity_x_burst'] = features['velocity_score'] * features['burst_ratio']
        
        feature_list.append(features)
    
    df_features = pd.DataFrame(feature_list).fillna(0)
    
    print(f"\nç‰¹å¾µå·¥ç¨‹å®Œæˆ!")
    print(f"ç¸½ç‰¹å¾µæ•¸: {len(df_features.columns) - 3}")  # æ¸›å»acct, label, ref_date
    
    return df_features


# ==================== ç¬¬äº”éƒ¨åˆ†ï¼šé«˜ç´šé›†æˆæ¨¡å‹ ====================

def train_breakthrough_models(X_train, y_train, X_val, y_val):
    """è¨“ç·´çªç ´æ€§é›†æˆæ¨¡å‹"""
    
    print("\n" + "="*80)
    print("è¨“ç·´é«˜ç´šé›†æˆæ¨¡å‹")
    print("="*80)
    
    models = {}
    predictions = {}
    
    # æ¨¡å‹1: LightGBMï¼ˆå„ªåŒ–ç‰ˆï¼‰
    print("\n[1/3] è¨“ç·´ LightGBM...")
    scale_pos_weight = (y_train == 0).sum() / ((y_train == 1).sum() + 1e-8)
    
    lgb_params = {
        'objective': 'binary',
        'boosting_type': 'gbdt',
        'metric': 'auc',
        'n_estimators': 2000,
        'learning_rate': 0.03,
        'num_leaves': 63,
        'max_depth': -1,
        'min_child_samples': 20,
        'subsample': 0.8,
        'subsample_freq': 1,
        'colsample_bytree': 0.8,
        'reg_alpha': 0.1,
        'reg_lambda': 0.1,
        'scale_pos_weight': scale_pos_weight,
        'random_state': RANDOM_SEED,
        'n_jobs': -1,
        'verbose': -1
    }
    
    lgb_model = lgb.LGBMClassifier(**lgb_params)
    lgb_model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        callbacks=[lgb.early_stopping(150), lgb.log_evaluation(0)]
    )
    models['lgb'] = lgb_model
    predictions['lgb'] = lgb_model.predict_proba(X_val)[:, 1]
    
    lgb_auc = roc_auc_score(y_val, predictions['lgb'])
    print(f"LightGBM - Val AUC: {lgb_auc:.4f}")
    
    # æ¨¡å‹2: XGBoostï¼ˆå„ªåŒ–ç‰ˆï¼‰
    print("\n[2/3] è¨“ç·´ XGBoost...")
    xgb_params = {
        'objective': 'binary:logistic',
        'eval_metric': 'auc',
        'n_estimators': 2000,
        'learning_rate': 0.03,
        'max_depth': 8,
        'min_child_weight': 3,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'gamma': 0.1,
        'reg_alpha': 0.1,
        'reg_lambda': 1.0,
        'scale_pos_weight': scale_pos_weight,
        'random_state': RANDOM_SEED,
        'n_jobs': -1,
        'tree_method': 'hist'
    }
    
    xgb_model = xgb.XGBClassifier(**xgb_params)
    xgb_model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        early_stopping_rounds=150,
        verbose=0
    )
    models['xgb'] = xgb_model
    predictions['xgb'] = xgb_model.predict_proba(X_val)[:, 1]
    
    xgb_auc = roc_auc_score(y_val, predictions['xgb'])
    print(f"XGBoost - Val AUC: {xgb_auc:.4f}")
    
    # æ¨¡å‹3: LightGBM with different paramsï¼ˆå¤šæ¨£æ€§ï¼‰
    print("\n[3/3] è¨“ç·´ LightGBM-Diverse...")
    lgb_params2 = {
        **lgb_params,
        'num_leaves': 127,
        'learning_rate': 0.02,
        'subsample': 0.7,
        'colsample_bytree': 0.7
    }
    
    lgb_model2 = lgb.LGBMClassifier(**lgb_params2)
    lgb_model2.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        callbacks=[lgb.early_stopping(150), lgb.log_evaluation(0)]
    )
    models['lgb2'] = lgb_model2
    predictions['lgb2'] = lgb_model2.predict_proba(X_val)[:, 1]
    
    lgb2_auc = roc_auc_score(y_val, predictions['lgb2'])
    print(f"LightGBM-Diverse - Val AUC: {lgb2_auc:.4f}")
    
    return models, predictions


# ==================== ä¸»ç¨‹åº ====================

def main():
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                           â•‘
    â•‘     ğŸš€ BREAKTHROUGH MODEL - Target F1 = 0.25+          â•‘
    â•‘                                                           â•‘
    â•‘  åŸºæ–¼æ·±åº¦åˆ†æä½ çš„8æ¬¡æäº¤æ­·å²é–‹ç™¼çš„é©å‘½æ€§è§£æ±ºæ–¹æ¡ˆ      â•‘
    â•‘                                                           â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    # åŠ è¼‰æ•¸æ“š
    print("\n[éšæ®µ1/6] åŠ è¼‰æ•¸æ“š...")
    df_trans = pd.read_csv('acct_transaction.csv')
    df_alert = pd.read_csv('acct_alert.csv')
    df_predict = pd.read_csv('acct_predict.csv')
    
    print(f"âœ“ äº¤æ˜“è¨˜éŒ„: {len(df_trans):,}")
    print(f"âœ“ è­¦ç¤ºå¸³æˆ¶: {len(df_alert):,}")
    print(f"âœ“ å¾…é æ¸¬: {len(df_predict):,}")
    
    # å‰µå»ºè¨“ç·´æ¨£æœ¬
    print("\n[éšæ®µ2/6] å‰µå»ºè¨“ç·´æ¨£æœ¬...")
    positive_samples = [(row['acct'], row['event_date'], 1) 
                        for _, row in df_alert.iterrows()]
    
    all_accts = np.union1d(df_trans['from_acct'].unique(), df_trans['to_acct'].unique())
    alert_accts = set(df_alert['acct'])
    non_alert_accts = np.setdiff1d(all_accts, list(alert_accts))
    
    num_neg = min(len(positive_samples) * NEG_POS_RATIO, len(non_alert_accts))
    selected_neg_accts = np.random.choice(non_alert_accts, num_neg, replace=False)
    
    negative_samples = []
    for acct in tqdm(selected_neg_accts, desc="æ¡æ¨£è² æ¨£æœ¬"):
        dates = df_trans[
            (df_trans['from_acct'] == acct) | (df_trans['to_acct'] == acct)
        ]['txn_date'].unique()
        if len(dates) > 1:
            # é¿å…æ•¸æ“šæ´©æ¼ï¼šåªé¸æ“‡ä¸­é–“çš„æ—¥æœŸ
            valid_dates = dates[(dates > dates.min()) & (dates < dates.max())]
            if len(valid_dates) > 0:
                pseudo_date = np.random.choice(valid_dates)
                negative_samples.append((acct, pseudo_date, 0))
    
    all_samples = positive_samples + negative_samples
    print(f"âœ“ ç¸½æ¨£æœ¬: {len(all_samples)} (æ­£:{len(positive_samples)}, è² :{len(negative_samples)})")
    
    # ç‰¹å¾µå·¥ç¨‹
    print("\n[éšæ®µ3/6] é©å‘½æ€§ç‰¹å¾µå·¥ç¨‹...")
    df_features = engineer_breakthrough_features(df_trans, df_alert, all_samples)
    
    # æ™‚é–“åˆ†å‰²ï¼ˆé¿å…æ•¸æ“šæ´©æ¼ï¼‰
    print("\n[éšæ®µ4/6] æ™‚é–“åˆ†å‰²é©—è­‰...")
    TIME_SPLIT = 90
    train_mask = df_features['ref_date'] <= TIME_SPLIT
    val_mask = df_features['ref_date'] > TIME_SPLIT
    
    feature_cols = [col for col in df_features.columns 
                    if col not in ['acct', 'label', 'ref_date']]
    
    X_train = df_features.loc[train_mask, feature_cols].values
    y_train = df_features.loc[train_mask, 'label'].values
    X_val = df_features.loc[val_mask, feature_cols].values
    y_val = df_features.loc[val_mask, 'label'].values
    
    print(f"âœ“ è¨“ç·´é›†: {len(X_train)} (æ­£æ¨£æœ¬ç‡: {y_train.mean():.4f})")
    print(f"âœ“ é©—è­‰é›†: {len(X_val)} (æ­£æ¨£æœ¬ç‡: {y_val.mean():.4f})")
    
    # æ¨™æº–åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_val_scaled = scaler.transform(X_val)
    
    # è¨“ç·´æ¨¡å‹
    print("\n[éšæ®µ5/6] è¨“ç·´é›†æˆæ¨¡å‹...")
    models, val_predictions = train_breakthrough_models(
        X_train_scaled, y_train, X_val_scaled, y_val
    )
    
    # Ensembleæ¬Šé‡å„ªåŒ–
    print("\nè¨ˆç®—æœ€å„ªensembleæ¬Šé‡...")
    aucs = {name: roc_auc_score(y_val, pred) 
            for name, pred in val_predictions.items()}
    
    total_auc = sum(aucs.values())
    weights = {name: auc / total_auc for name, auc in aucs.items()}
    
    print("\nEnsembleæ¬Šé‡:")
    for name, weight in weights.items():
        print(f"  {name}: {weight:.4f} (AUC: {aucs[name]:.4f})")
    
    # Ensembleé æ¸¬
    ensemble_val_pred = sum(weights[name] * val_predictions[name] 
                            for name in val_predictions.keys())
    
    # æ‰¾æœ€ä½³é–¾å€¼
    print("\nå°‹æ‰¾æœ€ä½³é–¾å€¼...")
    best_f1 = 0
    best_threshold = 0.5
    
    for threshold in np.arange(0.05, 0.95, 0.01):
        pred_labels = (ensemble_val_pred > threshold).astype(int)
        f1 = f1_score(y_val, pred_labels)
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
    
    print(f"\nâœ“ æœ€ä½³é©—è­‰F1: {best_f1:.6f}")
    print(f"âœ“ æœ€ä½³é–¾å€¼: {best_threshold:.4f}")
    print(f"âœ“ é©—è­‰AUC: {roc_auc_score(y_val, ensemble_val_pred):.4f}")
    
    # æ¸¬è©¦é›†é æ¸¬
    print("\n[éšæ®µ6/6] æ¸¬è©¦é›†é æ¸¬...")
    test_accts = df_predict['acct'].tolist()
    max_date = df_trans['txn_date'].max()
    test_samples = [(acct, max_date, 0) for acct in test_accts]
    
    df_test_features = engineer_breakthrough_features(df_trans, df_alert, test_samples)
    X_test = df_test_features[feature_cols].values
    X_test_scaled = scaler.transform(X_test)
    
    # Ensembleé æ¸¬
    test_predictions = {}
    for name, model in models.items():
        test_predictions[name] = model.predict_proba(X_test_scaled)[:, 1]
    
    ensemble_test_pred = sum(weights[name] * test_predictions[name] 
                            for name in test_predictions.keys())
    
    final_labels = (ensemble_test_pred > best_threshold).astype(int)
    
    # ä¿å­˜çµæœ
    submission = pd.DataFrame({
        'acct': test_accts,
        'label': final_labels
    })
    submission.to_csv('submission_breakthrough.csv', index=False)
    
    print("\n" + "="*80)
    print("âœ… å®Œæˆ!")
    print("="*80)
    print(f"æäº¤æ–‡ä»¶: submission_breakthrough.csv")
    print(f"é©—è­‰F1: {best_f1:.6f}")
    print(f"é æ¸¬åˆ†å¸ƒ: {pd.Series(final_labels).value_counts().to_dict()}")
    print(f"é™½æ€§ç‡: {final_labels.mean()*100:.2f}%")
    print("\né—œéµæ”¹é€²:")
    print("  âœ“ ç¶²çµ¡é¢¨éšªå‚³æ’­ç®—æ³•")
    print("  âœ“ æ™‚åºæ¨¡å¼ç‰¹å¾µ")
    print("  âœ“ é ˜åŸŸçŸ¥è­˜ç‰¹å¾µ")
    print("  âœ“ ä¸‰æ¨¡å‹æ·±åº¦ensemble")
    print("  âœ“ é¿å…æ•¸æ“šæ´©æ¼çš„åš´æ ¼æ™‚é–“åˆ†å‰²")
    print("="*80)


if __name__ == "__main__":
    main()
